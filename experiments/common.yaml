dataset:
    sequence_len: 256
    batch_size: 64
    alphabet_size: 16
    mem_tokens: 16
    selective: False

trainer:
    steps: 2
    test_steps: 10
    logging_steps: 100
    optimizer_class: torch.optim.AdamW
    optimizer_params:
        lr: 0.0001
    device: "cuda"

model:
  input_dim: 16
  hid_dim: 64
  model_dim: 64
  output_dim: 16
  n_layers: 2